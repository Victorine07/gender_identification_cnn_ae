{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage of VGG19 for gender identification\n",
    "\n",
    "    In this section, we are going to use the same training, validation and testing set and just change the model from VGG16 to VGG19 and evaluate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# librairies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\n",
    "\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80142336/80134624 [==============================] - 2s 0us/step\n",
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 178, 218, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 178, 218, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 178, 218, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 89, 109, 64)       0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 89, 109, 128)      73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 89, 109, 128)      147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 44, 54, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 44, 54, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 44, 54, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 44, 54, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 44, 54, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 22, 27, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 22, 27, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 22, 27, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 22, 27, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 22, 27, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 11, 13, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 11, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 11, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 11, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 11, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 5, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f87dae45190> False\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f87dac14490> False\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f87daea9e90> False\n",
      "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f87daf2acd0> False\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f87daf3bfd0> False\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f87daf64d50> False\n",
      "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f87daf6ee10> False\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f87daf3bb10> False\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f87daf77f10> False\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f87daf80c90> False\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f87daf805d0> False\n",
      "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f87daf77610> False\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f87daf73e10> False\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f87daf94950> False\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f87daf8a210> False\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f87daf9fd90> False\n",
      "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f87dafacd90> False\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f87dafa06d0> False\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f87daf8d810> True\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f87dafb1b90> True\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f87dafbd890> True\n",
      "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f87dafaef90> True\n",
      "<tensorflow.python.keras.layers.pooling.GlobalAveragePooling2D object at 0x7f87daf98610> True\n"
     ]
    }
   ],
   "source": [
    "num_classes=2\n",
    "\n",
    "vgg=VGG19(include_top=False, pooling='avg', weights='imagenet',input_shape=(178, 218, 3))\n",
    "vgg.summary()\n",
    "\n",
    "# Freeze the layers except the last 2 layers\n",
    "for layer in vgg.layers[:-5]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Check the trainable status of the individual layers\n",
    "for layer in vgg.layers:\n",
    "    print(layer, layer.trainable)\n",
    "    \n",
    "\n",
    "# Create the model\n",
    "model = models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg19 (Functional)           (None, 512)               20024384  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 20,090,818\n",
      "Trainable params: 7,145,602\n",
      "Non-trainable params: 12,945,216\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Add the vgg convolutional base model\n",
    "model.add(vgg)\n",
    " \n",
    "# Add new layers\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# compiling the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# use early stopping to optimally terminate training through callbacks\n",
    "es=EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
    "\n",
    "# save best model automatically\n",
    "mc= ModelCheckpoint('./CNN/Gender ID/VGG19/best_model_2_vgg19.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "cb_list=[es,mc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32000 images belonging to 2 classes.\n",
      "Found 7998 images belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "2667/2667 [==============================] - 19934s 7s/step - loss: 0.1186 - accuracy: 0.9584 - val_loss: 0.1171 - val_accuracy: 0.9552\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11711, saving model to ./CNN/Gender ID/VGG19/best_model_2_vgg19.h5\n",
      "Epoch 2/5\n",
      "2667/2667 [==============================] - 56928s 21s/step - loss: 0.1072 - accuracy: 0.9639 - val_loss: 0.0718 - val_accuracy: 0.9751\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11711 to 0.07182, saving model to ./CNN/Gender ID/VGG19/best_model_2_vgg19.h5\n",
      "Epoch 3/5\n",
      "2667/2667 [==============================] - 15109s 6s/step - loss: 0.0857 - accuracy: 0.9708 - val_loss: 0.1094 - val_accuracy: 0.9630\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.07182\n",
      "Epoch 4/5\n",
      "2667/2667 [==============================] - 10369s 4s/step - loss: 0.0783 - accuracy: 0.9750 - val_loss: 0.1218 - val_accuracy: 0.9584\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.07182\n",
      "Epoch 00004: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f872077b590>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "\n",
    "data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "\n",
    "train_generator = data_generator.flow_from_directory(\n",
    "        './data/Celeb_sets/train/',\n",
    "        target_size=(178, 218),\n",
    "        batch_size=12,\n",
    "        class_mode='categorical')\n",
    "\n",
    "\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "        './data/Celeb_sets/valid/',\n",
    "        target_size=(178, 218),\n",
    "        batch_size=12,\n",
    "        class_mode='categorical')\n",
    "\n",
    "\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        epochs=5,\n",
    "        steps_per_epoch=2667,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=667, callbacks=cb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/vickyyounang/Documents/PHD/winter2021/deep_learning/project_&_topic/Project/code/CNN/Gender ID/VGG19'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "root ='/Users/vickyyounang/Documents/PHD/winter2021/deep_learning/project_&_topic/Project/code/'\n",
    "test_path=   './data/Celeb_sets/test/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 2 classes.\n",
      "1000/1000 [==============================] - 209s 208ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load a saved model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# changing directory to the best model saved\n",
    "#os.chdir('./CNN/Gender ID/VGG19')\n",
    "saved_model = load_model('best_model_2_vgg19.h5')\n",
    "\n",
    "# generate data for test set of images\n",
    "test_generator = data_generator.flow_from_directory(\n",
    "        root+test_path,\n",
    "        target_size=(178, 218),\n",
    "        batch_size=1,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "# obtain predicted activation values for the last dense layer\n",
    "test_generator.reset()\n",
    "pred=saved_model.predict_generator(test_generator, verbose=1, steps=1000)\n",
    "# determine the maximum activation value for each sample\n",
    "predicted_class_indices=np.argmax(pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total accuracy =  0.977\n",
      "male accuracy =  0.968\n",
      "female accuracy =  0.986\n"
     ]
    }
   ],
   "source": [
    "# label each predicted value to correct gender\n",
    "labels = (test_generator.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions = [labels[k] for k in predicted_class_indices]\n",
    "\n",
    "# format file names to simply male or female\n",
    "filenames=test_generator.filenames\n",
    "\n",
    "#print(filenames)\n",
    "\n",
    "filenz=[0]\n",
    "for i in range(0,len(filenames)):\n",
    "    filenz.append(filenames[i].split('/')[0])\n",
    "filenz=filenz[1:]\n",
    "\n",
    "# determine the test set accuracy\n",
    "match=[]\n",
    "match_ml=[]\n",
    "match_fm=[]\n",
    "\n",
    "for i in range(0,len(filenames)):\n",
    "    match.append(filenz[i]==predictions[i])\n",
    "    if filenz[i]=='male':\n",
    "        match_ml.append(filenz[i]==predictions[i])\n",
    "    if filenz[i]=='female':\n",
    "        match_fm.append(filenz[i]==predictions[i])\n",
    "    \n",
    "print('total accuracy = ', match.count(True)/1000)\n",
    "print('male accuracy = ', match_ml.count(True)/500)\n",
    "print('female accuracy = ', match_fm.count(True)/500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing the results into a csv file\n",
    "\n",
    "results=pd.DataFrame({\"Filename\":filenz,\"Predictions\":predictions})\n",
    "\n",
    "results.to_csv(\"GenderID_VGG19_test_results.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/vickyyounang/Documents/PHD/winter2021/deep_learning/project_&_topic/Project/code/CNN/Gender ID/VGG19'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vickyyounang/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 4s 354ms/step\n"
     ]
    }
   ],
   "source": [
    "# predict for pictures of children\n",
    "test_generator = data_generator.flow_from_directory(\n",
    "        root+'data/Celeb_sets/test-me',\n",
    "        target_size=(178, 218),\n",
    "        batch_size=1,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "\n",
    "# obtain predicted activation values for the last dense layer\n",
    "test_generator.reset()\n",
    "#print(len(test_generator))\n",
    "pred=saved_model.predict_generator(test_generator, verbose=1, steps=10)\n",
    "# determine the maximum activation value for each sample\n",
    "predicted_class_indices=np.argmax(pred,axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['female/img_1.jpg', 'female/img_2.jpg', 'female/img_3.jpg', 'female/img_4.jpg', 'female/img_5.jpg', 'male/img_1.jpg', 'male/img_2.jpg', 'male/img_3.jpg', 'male/img_4.jpg', 'male/img_5.jpg']\n",
      "['female', 'female', 'female', 'female', 'female', 'female', 'male', 'male', 'male', 'male']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label each predicted value to correct gender\n",
    "labels = (test_generator.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions = [labels[k] for k in predicted_class_indices]\n",
    "\n",
    "#print(len(labels), len(predictions))\n",
    "\n",
    "# format file names to simply male or female\n",
    "filenames=test_generator.filenames\n",
    "\n",
    "print(filenames)\n",
    "\n",
    "filenz=[0]\n",
    "for i in range(0,len(filenames)):\n",
    "    filenz.append(filenames[i].split('/')[0])\n",
    "filenz=filenz[1:]\n",
    "\n",
    "print(predictions)\n",
    "#print(filenz)\n",
    "\n",
    "# determine the test set accuracy\n",
    "match=[]\n",
    "for i in range(0,len(filenames)):\n",
    "    match.append(filenz[i]==predictions[i])\n",
    "    \n",
    "#print(match)\n",
    "match.count(True)/len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, True, True, True, False, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
