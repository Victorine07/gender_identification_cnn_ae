{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gender identification with celebA and VGG16 pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dataset\n",
    "The CelebA dataset contains over 200K images of celebrities labeled with 20 attributes including gender. The images are from the shoulders up, so most of the information is in the facial features and hair style.\n",
    "\n",
    "\n",
    "For our experiment, we will be using 60k images with 20 selected attributes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Feature and data extraction/preparation\n",
    "\n",
    "    Weâ€™re going to use the VGG16 pretrained model and fine tune it to best identify gender from the celebrity images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# librairies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image_id' '5_o_Clock_Shadow' 'Arched_Eyebrows' 'Attractive'\n",
      " 'Bags_Under_Eyes' 'Bald' 'Bangs' 'Big_Lips' 'Big_Nose' 'Black_Hair'\n",
      " 'Blond_Hair' 'Blurry' 'Brown_Hair' 'Bushy_Eyebrows' 'Chubby'\n",
      " 'Double_Chin' 'Eyeglasses' 'Goatee' 'Gray_Hair' 'Heavy_Makeup'\n",
      " 'High_Cheekbones' 'Male' 'Mouth_Slightly_Open' 'Mustache' 'Narrow_Eyes'\n",
      " 'No_Beard' 'Oval_Face' 'Pale_Skin' 'Pointy_Nose' 'Receding_Hairline'\n",
      " 'Rosy_Cheeks' 'Sideburns' 'Smiling' 'Straight_Hair' 'Wavy_Hair'\n",
      " 'Wearing_Earrings' 'Wearing_Hat' 'Wearing_Lipstick' 'Wearing_Necklace'\n",
      " 'Wearing_Necktie' 'Young']\n",
      "(202599, 41)\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "df=pd.read_csv('data/list_attr_celeba.csv')\n",
    "\n",
    "df.head()\n",
    "\n",
    "print(df.columns.values)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>000003.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>000007.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>000008.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     image_id  Male\n",
       "2  000003.jpg     1\n",
       "6  000007.jpg     1\n",
       "7  000008.jpg     1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get labels for either gender\n",
    "male=df[df['Male']==1][0:20000][['image_id', 'Male']]\n",
    "\n",
    "female=df[df['Male']==-1][0:20000][['image_id','Male']]\n",
    "\n",
    "male.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45740    045741.jpg\n",
       "11812    011813.jpg\n",
       "39695    039696.jpg\n",
       "Name: image_id, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting train, test sets for either gender\n",
    "m_train_X, m_test_X, train_y, test_y = train_test_split(male['image_id'],male['Male'], random_state = 0, test_size=.2)\n",
    "f_train_X, f_test_X, train_y, test_y = train_test_split(female['image_id'],female['Male'], random_state = 0, test_size=.2)\n",
    "\n",
    "m_test_X.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "# creating folder to structure the data\n",
    "origin_path= './data/img_align_celeba/'\n",
    "train_path=  './data/Celeb_sets/train/'\n",
    "valid_path=  './data/Celeb_sets/valid/'\n",
    "test_path=   './data/Celeb_sets/test/'\n",
    "fm='female/'\n",
    "ml='male/'\n",
    "\n",
    "# creating the directories\n",
    "os.makedirs(train_path+ml)\n",
    "os.makedirs(valid_path+ml)\n",
    "os.makedirs(train_path+fm)\n",
    "os.makedirs(valid_path+fm)\n",
    "\n",
    "\n",
    "for file in m_train_X:\n",
    "    #os.makedirs(origin_path+train_path+ml+file)\n",
    "    shutil.copy(origin_path+file, train_path+ml+file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_test_X = m_test_X.iloc[2:]\n",
    "\n",
    "for file in m_test_X:\n",
    "    #os.makedirs(origin_path+valid_path+ml+file)\n",
    "    shutil.copy(origin_path+file, valid_path+ml+file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for file in f_train_X:\n",
    "    #os.makedirs(origin_path+train_path+fm+file)\n",
    "    shutil.copy(origin_path+file, train_path+fm+file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for file in f_test_X:\n",
    "    #os.makedirs(origin_path+valid_path+fm+file)\n",
    "    shutil.copy(origin_path+file, valid_path+fm+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct seperate test set\n",
    "test_m=df[df['Male']==1][-500:]\n",
    "test_m=test_m.loc[:,'image_id']\n",
    "test_f=df[df['Male']==-1][-500:]\n",
    "test_f=test_f.loc[:,'image_id']\n",
    "\n",
    "test_path='./data/Celeb_sets/test/'\n",
    "os.makedirs(test_path+ml)\n",
    "os.makedirs(test_path+fm)\n",
    "\n",
    "for file in test_m:\n",
    "    shutil.copy(origin_path+file, test_path+ml+file)\n",
    "\n",
    "for file in test_f:\n",
    "    shutil.copy(origin_path+file, test_path+fm+file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\n",
    "\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 178, 218, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 178, 218, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 178, 218, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 89, 109, 64)       0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 89, 109, 128)      73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 89, 109, 128)      147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 44, 54, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 44, 54, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 44, 54, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 44, 54, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 22, 27, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 22, 27, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 22, 27, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 22, 27, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 11, 13, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 11, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 11, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 11, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 5, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f896587f390> False\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f895f9bc3d0> False\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f8960d52c10> False\n",
      "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f8960dc6f50> False\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f8960dd1c50> False\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f8960dcd550> False\n",
      "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f8960ddaf50> False\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f8960de5e10> False\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f8960de9ed0> False\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f8960ddfa50> False\n",
      "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f8960df6b10> False\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f8960df9e90> False\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f8960e6da90> False\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f8960dda7d0> False\n",
      "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f8960e75e50> False\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f8960e7c0d0> True\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f8960e75090> True\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f8960e81f90> True\n",
      "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f895e603050> True\n",
      "<tensorflow.python.keras.layers.pooling.GlobalAveragePooling2D object at 0x7f8960e83590> True\n"
     ]
    }
   ],
   "source": [
    "num_classes=2\n",
    "\n",
    "vgg=VGG16(include_top=False, pooling='avg', weights='imagenet',input_shape=(178, 218, 3))\n",
    "vgg.summary()\n",
    "\n",
    "# Freeze the layers except the last 2 layers\n",
    "for layer in vgg.layers[:-5]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Check the trainable status of the individual layers\n",
    "for layer in vgg.layers:\n",
    "    print(layer, layer.trainable)\n",
    "    \n",
    "\n",
    "# Create the model\n",
    "model = models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Functional)           (None, 512)               14714688  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 14,781,122\n",
      "Trainable params: 7,145,602\n",
      "Non-trainable params: 7,635,520\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Add the vgg convolutional base model\n",
    "model.add(vgg)\n",
    " \n",
    "# Add new layers\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# compiling the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# use early stopping to optimally terminate training through callbacks\n",
    "es=EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
    "\n",
    "# save best model automatically\n",
    "mc= ModelCheckpoint('./CNN/Gender ID/best_model_2.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "cb_list=[es,mc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32000 images belonging to 2 classes.\n",
      "Found 7998 images belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "2667/2667 [==============================] - 15723s 6s/step - loss: 0.1183 - accuracy: 0.9573 - val_loss: 0.0611 - val_accuracy: 0.9770\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.08378 to 0.06112, saving model to ./CNN/Gender ID/best_model_2.h5\n",
      "Epoch 2/5\n",
      "2667/2667 [==============================] - 17181s 6s/step - loss: 0.0871 - accuracy: 0.9700 - val_loss: 0.1069 - val_accuracy: 0.9630\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.06112\n",
      "Epoch 3/5\n",
      "2667/2667 [==============================] - 53601s 20s/step - loss: 0.0931 - accuracy: 0.9676 - val_loss: 3.9196 - val_accuracy: 0.9789\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.06112\n",
      "Epoch 00003: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f88b2cf7b90>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "\n",
    "data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "\n",
    "train_generator = data_generator.flow_from_directory(\n",
    "        './data/Celeb_sets/train/',\n",
    "        target_size=(178, 218),\n",
    "        batch_size=12,\n",
    "        class_mode='categorical')\n",
    "\n",
    "\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "        './data/Celeb_sets/valid/',\n",
    "        target_size=(178, 218),\n",
    "        batch_size=12,\n",
    "        class_mode='categorical')\n",
    "\n",
    "\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        epochs=5,\n",
    "        steps_per_epoch=2667,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=667, callbacks=cb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/vickyyounang/Documents/PHD/winter2021/deep_learning/project_&_topic/Project/code/CNN/Gender ID'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n",
    "#root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.engine.sequential.Sequential object at 0x7f88b3103610>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load a saved model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# changing directory to the best model saved\n",
    "os.chdir('./CNN/Gender ID/')\n",
    "saved_model = load_model('best_model_2.h5')\n",
    "\n",
    "print(saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "root ='/Users/vickyyounang/Documents/PHD/winter2021/deep_learning/project_&_topic/Project/code/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 2 classes.\n",
      "1000/1000 [==============================] - 182s 181ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#os.chdir(root)\n",
    "\n",
    "# generate data for test set of images\n",
    "test_generator = data_generator.flow_from_directory(\n",
    "        test_path,\n",
    "        target_size=(178, 218),\n",
    "        batch_size=1,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "# obtain predicted activation values for the last dense layer\n",
    "test_generator.reset()\n",
    "pred=saved_model.predict_generator(test_generator, verbose=1, steps=1000)\n",
    "# determine the maximum activation value for each sample\n",
    "predicted_class_indices=np.argmax(pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label each predicted value to correct gender\n",
    "labels = (test_generator.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions = [labels[k] for k in predicted_class_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['female', 'female', 'female', 'female', 'female', 'female', 'female', 'female', 'female', 'female']\n"
     ]
    }
   ],
   "source": [
    "print(predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format file names to simply male or female\n",
    "filenames=test_generator.filenames\n",
    "\n",
    "#print(filenames)\n",
    "\n",
    "filenz=[0]\n",
    "for i in range(0,len(filenames)):\n",
    "    filenz.append(filenames[i].split('/')[0])\n",
    "filenz=filenz[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total accuracy =  0.98\n",
      "male accuracy =  0.982\n",
      "female accuracy =  0.978\n"
     ]
    }
   ],
   "source": [
    "# determine the test set accuracy\n",
    "match=[]\n",
    "match_ml=[]\n",
    "match_fm=[]\n",
    "\n",
    "for i in range(0,len(filenames)):\n",
    "    match.append(filenz[i]==predictions[i])\n",
    "    if filenz[i]=='male':\n",
    "        match_ml.append(filenz[i]==predictions[i])\n",
    "    if filenz[i]=='female':\n",
    "        match_fm.append(filenz[i]==predictions[i])\n",
    "    \n",
    "print('total accuracy = ', match.count(True)/1000)\n",
    "print('male accuracy = ', match_ml.count(True)/500)\n",
    "print('female accuracy = ', match_fm.count(True)/500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['female', 'female', 'female', 'female', 'female', 'male', 'male', 'male', 'male', 'male']\n",
      "['female', 'female', 'female', 'female', 'female', 'female', 'male', 'male', 'male', 'male']\n"
     ]
    }
   ],
   "source": [
    "print(filenz[:10])\n",
    "print(predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'pandas.core.series.Series'>\n",
      "167 False\n",
      "168 False\n",
      "176 False\n",
      "199 False\n",
      "208 False\n",
      "211 False\n",
      "216 False\n",
      "411 False\n",
      "437 False\n",
      "456 False\n",
      "470 False\n",
      "592 False\n",
      "719 False\n",
      "731 False\n",
      "735 False\n",
      "766 False\n",
      "786 False\n",
      "800 False\n",
      "820 False\n",
      "956 False\n"
     ]
    }
   ],
   "source": [
    "results=pd.DataFrame({\"Filename\":filenz,\"Predictions\":predictions})\n",
    "\n",
    "print(type(filenz), type(pd.Series(predictions)))\n",
    "\n",
    "# let's see the false predictions\n",
    "\n",
    "for i in range(1000):\n",
    "    if filenz[i] != predictions[i]:\n",
    "        print(i+1, False)\n",
    "\n",
    "\n",
    "#pd.Series(filenz).str.match(pd.Series(predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results.to_csv(\"GenderID_VGG16_test_results.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 images belonging to 2 classes.\n",
      "10/10 [==============================] - 2s 193ms/step\n"
     ]
    }
   ],
   "source": [
    "# predict for pictures of children\n",
    "test_generator = data_generator.flow_from_directory(\n",
    "        root+'data/Celeb_sets/test-me',\n",
    "        target_size=(178, 218),\n",
    "        batch_size=1,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "\n",
    "# obtain predicted activation values for the last dense layer\n",
    "test_generator.reset()\n",
    "#print(len(test_generator))\n",
    "pred=saved_model.predict_generator(test_generator, verbose=1, steps=10)\n",
    "# determine the maximum activation value for each sample\n",
    "predicted_class_indices=np.argmax(pred,axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['female/img_1.jpg', 'female/img_2.jpg', 'female/img_3.jpg', 'female/img_4.jpg', 'female/img_5.jpg', 'male/img_1.jpg', 'male/img_2.jpg', 'male/img_3.jpg', 'male/img_4.jpg', 'male/img_5.jpg']\n",
      "['female', 'female', 'female', 'female', 'female', 'female', 'male', 'male', 'male', 'male']\n",
      "['female', 'female', 'female', 'female', 'female', 'male', 'male', 'male', 'male', 'male']\n",
      "[True, True, True, True, True, False, True, True, True, True]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label each predicted value to correct gender\n",
    "labels = (test_generator.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions = [labels[k] for k in predicted_class_indices]\n",
    "\n",
    "#print(len(labels), len(predictions))\n",
    "\n",
    "# format file names to simply male or female\n",
    "filenames=test_generator.filenames\n",
    "\n",
    "print(filenames)\n",
    "\n",
    "filenz=[0]\n",
    "for i in range(0,len(filenames)):\n",
    "    filenz.append(filenames[i].split('/')[0])\n",
    "filenz=filenz[1:]\n",
    "\n",
    "print(predictions)\n",
    "print(filenz)\n",
    "\n",
    "# determine the test set accuracy\n",
    "match=[]\n",
    "for i in range(0,len(filenames)):\n",
    "    match.append(filenz[i]==predictions[i])\n",
    "    \n",
    "print(match)\n",
    "match.count(True)/len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['female/img_1.jpg', 'female/img_2.jpg', 'female/img_3.jpg', 'female/img_4.jpg', 'female/img_5.jpg', 'male/img_1.jpg', 'male/img_2.jpg', 'male/img_3.jpg', 'male/img_4.jpg', 'male/img_5.jpg']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
